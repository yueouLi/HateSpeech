{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ninitialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1390\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1391\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_processing_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseImageProcessor\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration_auto\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/image_processing_utils.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchFeature \u001b[39mas\u001b[39;00m BaseBatchFeature\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_transforms\u001b[39;00m \u001b[39mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ChannelDimension\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/image_transforms.py:47\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m _tf2\u001b[39m.\u001b[39menable()\n\u001b[0;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __operators__\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m eager_context\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m combinations\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m interim\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcombinations\u001b[39;00m \u001b[39mimport\u001b[39;00m env \u001b[39m# line: 456\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcombinations\u001b[39;00m \u001b[39mimport\u001b[39;00m generate \u001b[39m# line: 365\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/combinations.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m session\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute_lib\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m collective_util\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_device_ops \u001b[39mas\u001b[39;00m cross_device_ops_lib\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_device_utils\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/cross_device_ops.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m collective_util\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_device_utils\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m device_util\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/cross_device_utils.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m collective_util\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m values \u001b[39mas\u001b[39;00m value_lib\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m backprop_util\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/values.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m device_util\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute_lib\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m packed_distributed_variable \u001b[39mas\u001b[39;00m packed\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:206\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimpl\u001b[39;00m \u001b[39mimport\u001b[39;00m api \u001b[39mas\u001b[39;00m autograph\n\u001b[0;32m--> 206\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_ops\n\u001b[1;32m    207\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m collective_util\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_mode\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m iterator_ops\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m options \u001b[39mas\u001b[39;00m options_lib\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrackable\u001b[39;00m \u001b[39mimport\u001b[39;00m base \u001b[39mas\u001b[39;00m trackable\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaver\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSaverBuilder\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/training/saver.py:50\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrackable\u001b[39;00m \u001b[39mimport\u001b[39;00m base \u001b[39mas\u001b[39;00m trackable\n\u001b[0;32m---> 50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m py_checkpoint_reader\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m training_util\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pywrap_checkpoint_reader\u001b[39;00m \u001b[39mimport\u001b[39;00m CheckpointReader\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export\n",
      "\u001b[0;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/liyueou/Desktop/HSD/day3_hatespeech_detection_BERT/project_gaze4hate_bert_classification.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liyueou/Desktop/HSD/day3_hatespeech_detection_BERT/project_gaze4hate_bert_classification.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/liyueou/Desktop/HSD/day3_hatespeech_detection_BERT/project_gaze4hate_bert_classification.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer, AutoModel, AutoConfig, utils\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/liyueou/Desktop/HSD/day3_hatespeech_detection_BERT/project_gaze4hate_bert_classification.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1380\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1379\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1380\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1381\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1382\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1392\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1391\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1392\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1393\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1394\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1395\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ninitialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification,AutoTokenizer, AutoModel, AutoConfig, utils\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip show transformers\n",
    "#!pip install torch torchvision\n",
    "#!pip install -U scikit-learn\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sentence_file =  \"../Project2_Gaze4Hate/gaze4hate_sentences_to_annotate.xlsx\"\n",
    "orig_df = pd.read_excel(open(orig_sentence_file, 'rb'), sheet_name='stimuli') \n",
    "\n",
    "## remove the 4 familiarization sentences (last four)\n",
    "orig_df= orig_df.loc[orig_df['Cond_remark']!='fam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hate_w_high', 'no_hate_w', 'hate_w_med', 'hate_m', 'neutral_w',\n",
       "       'pos_w'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df.Cond_remark.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Cond_remark</th>\n",
       "      <th>ID</th>\n",
       "      <th>processed_assertion</th>\n",
       "      <th>H_P_mw</th>\n",
       "      <th>H_N_mw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hate_w_high</td>\n",
       "      <td>27</td>\n",
       "      <td>Frauen haben einen signifikant geringeren Durc...</td>\n",
       "      <td>[5,6]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>hate_w_high</td>\n",
       "      <td>26</td>\n",
       "      <td>Frauen sind psychisch nicht für komplexe Arbei...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>hate_w_high</td>\n",
       "      <td>16</td>\n",
       "      <td>Frauen haben im statistischen Mittel ein klein...</td>\n",
       "      <td>[7,8]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>hate_w_high</td>\n",
       "      <td>14</td>\n",
       "      <td>Frauenförderung hat fast nur Nachteile für die...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>hate_w_high</td>\n",
       "      <td>2</td>\n",
       "      <td>Frauen sind im Durchschnitt dümmer als Männer.</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[1,7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>pos_w</td>\n",
       "      <td>28</td>\n",
       "      <td>Gleichstellung der Geschlechter ist ein Traum ...</td>\n",
       "      <td>[6,7,8,9]</td>\n",
       "      <td>[99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>pos_w</td>\n",
       "      <td>32</td>\n",
       "      <td>Frauen in führenden Positionen haben häufig st...</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>pos_w</td>\n",
       "      <td>1</td>\n",
       "      <td>Frauen gehören in die Regierung.</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>pos_w</td>\n",
       "      <td>12</td>\n",
       "      <td>Frauen sollten ihren Traumberuf ausüben.</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "      <td>pos_w</td>\n",
       "      <td>7</td>\n",
       "      <td>Alles was eine Frau tun muss, ist ihre Wünsche...</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[99]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sno  Condition  Cond_remark  ID  \\\n",
       "0     1          1  hate_w_high  27   \n",
       "1     2          1  hate_w_high  26   \n",
       "2     3          1  hate_w_high  16   \n",
       "3     4          1  hate_w_high  14   \n",
       "4     5          1  hate_w_high   2   \n",
       "..  ...        ...          ...  ..   \n",
       "85   89          3        pos_w  28   \n",
       "86   90          3        pos_w  32   \n",
       "87   91          3        pos_w   1   \n",
       "88   92          3        pos_w  12   \n",
       "89   93          3        pos_w   7   \n",
       "\n",
       "                                  processed_assertion     H_P_mw H_N_mw  \n",
       "0   Frauen haben einen signifikant geringeren Durc...      [5,6]    [1]  \n",
       "1   Frauen sind psychisch nicht für komplexe Arbei...        [4]    [1]  \n",
       "2   Frauen haben im statistischen Mittel ein klein...      [7,8]    [1]  \n",
       "3   Frauenförderung hat fast nur Nachteile für die...        [5]    [1]  \n",
       "4      Frauen sind im Durchschnitt dümmer als Männer.        [5]  [1,7]  \n",
       "..                                                ...        ...    ...  \n",
       "85  Gleichstellung der Geschlechter ist ein Traum ...  [6,7,8,9]   [99]  \n",
       "86  Frauen in führenden Positionen haben häufig st...        [7]   [99]  \n",
       "87                   Frauen gehören in die Regierung.        [5]   [99]  \n",
       "88           Frauen sollten ihren Traumberuf ausüben.        [4]   [99]  \n",
       "89  Alles was eine Frau tun muss, ist ihre Wünsche...        [9]   [99]  \n",
       "\n",
       "[90 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initial six categories can be categorized into two as hate versus nohate\n",
    "orig_df.loc[orig_df['Cond_remark']=='hate_w_high', 'label']= 1 # hate\n",
    "orig_df.loc[orig_df['Cond_remark']=='no_hate_w', 'label']= 0 # nohate\n",
    "orig_df.loc[orig_df['Cond_remark']=='hate_w_med', 'label']= 1 # hate\n",
    "orig_df.loc[orig_df['Cond_remark']=='hate_m', 'label']= 1 # hate\n",
    "orig_df.loc[orig_df['Cond_remark']=='neutral_w', 'label']= 0 # nohate\n",
    "orig_df.loc[orig_df['Cond_remark']=='pos_w', 'label']= 0 # nohate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, encodings):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    masks = torch.tensor([[int(e > 0) for e in encodings]], device=device)\n",
    "    input = torch.tensor([encodings], dtype=torch.long, device=device)\n",
    "    # get model output\n",
    "    output = model(input, masks)\n",
    "    pred_id = torch.argmax(output[0], dim=1).item()\n",
    "    #pred_label = model.config.id2label[pred_id]\n",
    "\n",
    "    return pred_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(orig_df, bert_type):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if bert_type=='base':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")   \n",
    "        model = AutoModelForSequenceClassification.from_pretrained('dbmdz/bert-base-german-uncased', output_hidden_states=True, return_dict=True, output_attentions=True).to(device)\n",
    "\n",
    "    if bert_type==\"germeval18\" :   \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\",output_hidden_states=True, return_dict=True, output_attentions=True).to(device)   \n",
    "    \n",
    "\n",
    "    # get input\n",
    "\n",
    "    texts = orig_df['processed_assertion']\n",
    "    \n",
    "    # encode input\n",
    "    encodings = [tokenizer.encode(t) for t in texts]\n",
    "    \n",
    "    # predict testset\n",
    "    predictions = [predict(model, e) for e in tqdm(encodings)]\n",
    "    \n",
    "    #print(predictions)\n",
    "    # save preds\n",
    "    orig_df[bert_type+'_PREDS'] = predictions\n",
    "    orig_df.to_csv('gaze4hate_sentence_bert_predictions_'+ bert_type+'.csv', index=False)\n",
    "    \n",
    "    labels = orig_df['label']\n",
    "    #print(labels)\n",
    "\n",
    "     # evaluate predictions\n",
    "    print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   423,   366, 18014, 28385, 22226, 24187,  5926,  8791,\n",
      "         30938,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/90 [00:00<00:19,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,  3825,   187,   237,  9586, 18334,  1029,  5056,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   423,   197, 12597,  1085,   139,  8689, 30941,  1954,\n",
      "          2789,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  6344,   292,  1932,   435, 21736,  9586,   125,  2247,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   197,  5541, 12787,   105,   250,   946,   105,\n",
      "           552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7/90 [00:01<00:08,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   946,   278, 30543,   142,  3033,  5064, 28645,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,  7034, 19314,   142,  2595,  2247,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   367,   316,  1454,   143,   321,   167, 12787,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648, 28961,   536,  2140,   179,  1020,   153,  5456,   552,\n",
      "           103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12/90 [00:01<00:05, 15.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  125, 1603, 1744, 5892,  223, 5561,  207, 4165,  323,  187,  552,\n",
      "          103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  2152,   167,  3637,  2647,   143,   235,   271,   173,\n",
      "          2644, 18660,   334, 30938,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 21629,  1165,   321, 24026,  4968, 19634,   329,   946,   105,\n",
      "         19289, 30941,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102, 1648, 4826,  281, 6403,  806,  946,  105, 4826,  281, 2823,  552,\n",
      "          103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   238, 11702,  7485, 12452,   142,   127,  2679,   207,   167,\n",
      "         28081,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 16/90 [00:01<00:04, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102, 1648,  321, 9586, 9342, 7708,  237, 6940,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 22747,   127, 17114,   207,   238, 24026,   291, 28809,   140,\n",
      "           185,  1648,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   142, 16093,   166, 10792,   423,   792,  4027, 30948,\n",
      "         21617, 30937,   980,  5677,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648, 20197,   106,   142,   125,  3234,   383,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/90 [00:01<00:04, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  2152,   597, 11702, 17114,  7244,   216, 10360, 30938,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1177,   359,   238,   884,  1575,  1055,   806,   207,  1053,\n",
      "           946,   167, 25005,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  1260,  3342,  6446,  1091, 15739,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   946,   105,   143,  1648,   321,   829,  8403, 30942,   552,\n",
      "           103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 24/90 [00:01<00:03, 17.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102, 1648,  367,  316, 1083, 1129,  453, 6252,  181,  267,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   237, 10483,   143,  2152,   235,   224,   277,\n",
      "           237,  4035,  1470,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102, 1648, 1794,  173, 6307, 1260, 4585,  375,  281,  946,  105,  552,\n",
      "          103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  2391, 17114,  2152,   829, 10128,   318,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 28/90 [00:02<00:03, 17.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   829, 22610,   207,   238, 25159,   440,  2004,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  2206,  1055,   106,   197,  3508,  4031,   281,  2254,  6015,\n",
      "          9586,   597,  1129, 24026,   726,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   125, 17114,   321,   237,   788,   829,   806,   995,   237,\n",
      "          7034,  1582,   690,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 30596, 30112,  2254,   321, 16063,  1245,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 32/90 [00:02<00:03, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   233,   773,  1648, 11435, 30937,   143,   233,   773,   946,\n",
      "         18365,  5986,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   946,   105,  4035,  1470,   235,   185,  1648,   142,  5407,\n",
      "           310, 18398, 12568,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  7305,  1055, 24026,   353,   267,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  946,  105,  143, 1648,  321,  142,  343, 8714, 1905,  266,  453,\n",
      "         6252,  181,  276,  552,  103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 37/90 [00:02<00:02, 18.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   244,  7685,  4625,  5881, 16789,   410,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   423,  1630,  7233,   244,  3596, 25248, 24026,   179,\n",
      "         26810,   127,  2197,  1129,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   197,  5541, 21055, 30940,   250,   946,   105,\n",
      "           142, 28544, 21092,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   125, 30425,   223, 13590,   223, 23808,   207,   859,  1453,\n",
      "           210,  3836,  2005,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   367,   316,   235,  1761,   271,  1129,   143,  1905,\n",
      "         18660,   334, 30938,   806,   233,  1234,   142,   948,  1896,   552,\n",
      "           103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 42/90 [00:02<00:02, 19.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  8674,  1648, 15346,   321, 30878, 16076, 30948,   143,  9367,\n",
      "         13432,   407,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  946,  105,  321,  704, 8861,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   946,   105,   321, 12787,   105,   250,  1648,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  946,  105,  321,  126,  373, 2136,  454,  690,  250, 1648,  552,\n",
      "          103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 46/90 [00:03<00:02, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  676,  946,  105,  321, 3289,  296,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  946,  105,  321,  237,  125, 6239,  276,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   127,   946,  5970, 12979,   115,   173,  1898,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   946,   105, 20197,   106,   193,   179,   844,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50/90 [00:03<00:02, 18.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   946,   105,   321,   244,  2080, 22426, 30938, 16789,   410,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  946,  105,  663,  670, 2253,  530,  183, 1053, 4268,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   676,   946,   105,   321,  6115,  1667, 30937,  1129,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 25389,   158,  2480,   423,   366, 18014, 28385, 22226, 24187,\n",
      "          5926,  8791, 30938,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 54/90 [00:03<00:02, 17.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1129,   321,  3825,   187,   237,  9586, 18334,  1029,  5056,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1129,   423,   197, 12597,  1085,   139,  8689, 30941,  1954,\n",
      "          2789,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 21769,  2005,   292,  1932,   435, 21736,  9586,   125,  2247,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 13765,   321,   197,  5541, 12787,   105,   250,  1361,   302,\n",
      "         19225,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 58/90 [00:03<00:02, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  8567,   321,  1012, 30543,   142,  3033,  5064, 28645,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  7028,  1154, 23823,  7602,   321,  7034, 19314,   142,  3572,\n",
      "         16772,  3006,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  9628,  2480,   367,   316,  1454,   143,   321,   167, 12787,\n",
      "           552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 60/90 [00:04<00:01, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  3399, 28961,   410,  2140,   179,  1020,   153,  5456,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   125,  1603,   859, 12667,   223,  5561,   207,  4165,   323,\n",
      "           187,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 20032,  2855,  2152,   167,  3637,  2647,   143,   235,   271,\n",
      "           173,  2644, 18660,   334, 30938,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 62/90 [00:04<00:01, 15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  9146,   321, 24026,  4968, 19634,   329, 16907,   154,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  7119,  4826,   281,  6403,   806, 24154,   105,  4826,   281,\n",
      "          2823,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   139, 20886, 30941, 24792,  2297,   142,   127,  2679,   207,\n",
      "           167, 28081,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 66/90 [00:04<00:01, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  2330, 24993,   106,   321,  9586,  4459, 19916,   237,  6940,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 22747,   127, 17114,  2519,  7424,   792,  4027,   286,  1317,\n",
      "           552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1012,   142, 16093,   166, 10792,   423,   792,  4027, 30948,\n",
      "         21617, 30937,  4739,  2675, 30937,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [00:04<00:01, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 24109, 30937, 20197,   106,   142,   125,  3234,   383,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  3579, 10612,  2152,   597, 11702,   453, 25960,  7244,   216,\n",
      "         10360, 30938,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1177,   359,   139,  1501,  1575,  1055,   806,   207,   610,\n",
      "          6090,   167, 25005,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   423,   366, 18014, 28385,  3325, 30937,  9747,  2989,\n",
      "           584,   356,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 74/90 [00:05<00:01, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,  3825,   187,  9586, 18334,  1029,  5056,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   423,   197, 12597,  1085,   142,   366,  3888,   106,\n",
      "          3098, 18575,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102, 1648, 6344,  292, 1932,  435, 7233, 9586,  125, 2247,  552,  103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   197,  5541, 14091,   250,   946,   105,   552,\n",
      "           103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 78/90 [00:05<00:00, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,   946,   278,   197, 28581, 12196,  1280, 24026,\n",
      "          1175,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321, 22916,   106,   142,  2595,  2247,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   367,   316,  1177,   143,   321,   789, 12325,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102, 1648, 3839, 2140,  179, 1020,  153, 5456,  552,  103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 82/90 [00:05<00:00, 16.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 102,  125, 1603, 1744, 5892,  223, 5561,  207, 1337, 4344,  209,  552,\n",
      "          103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  2152,   237,   167,  3637,  2647,   143,   235,   271,\n",
      "           597,  4478, 18660,   334, 30938,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 21629,  1165,   321, 24026,  4968, 19634,   329, 13018,  5926,\n",
      "          3745, 30937,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 21078,   124,  4826,   281,  6403,   806,  1648,  4826,   281,\n",
      "          2823,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 86/90 [00:05<00:00, 17.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,   238,  6727,  7485, 12452,   142,   127,  2679,   207,   167,\n",
      "         28081,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   321,  9586,   676, 19916,   832,  6940,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102, 22747,   127, 17114,   207,   139,  6263,   127,  2247,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,   142, 16093,   166, 10792,   423,   792,  4027, 30948,\n",
      "          6246,   980,  5677,   552,   103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:05<00:00, 15.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648, 20197,   106,   142,   125,  2119,   552,   103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1648,  2152,  1053,  6263, 11435,   216, 10360, 30938,   552,\n",
      "           103]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[  102,  1177,   359,   238,   884,  1575,  1055,   806,   207,   597,\n",
      "          5557, 30937,   167, 25005,   552,   103]])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.90      0.69        50\n",
      "         1.0       0.50      0.12      0.20        40\n",
      "\n",
      "    accuracy                           0.56        90\n",
      "   macro avg       0.53      0.51      0.45        90\n",
      "weighted avg       0.53      0.56      0.47        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(orig_df, 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:05<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.78      0.68        50\n",
      "         1.0       0.58      0.38      0.45        40\n",
      "\n",
      "    accuracy                           0.60        90\n",
      "   macro avg       0.59      0.58      0.57        90\n",
      "weighted avg       0.59      0.60      0.58        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(orig_df, 'germeval18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>25291</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>25292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>25294</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>25295</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>25296</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0               0      3            0                   0        3      2   \n",
       "1               1      3            0                   3        0      1   \n",
       "2               2      3            0                   3        0      1   \n",
       "3               3      3            0                   2        1      1   \n",
       "4               4      6            0                   6        0      1   \n",
       "...           ...    ...          ...                 ...      ...    ...   \n",
       "24778       25291      3            0                   2        1      1   \n",
       "24779       25292      3            0                   1        2      2   \n",
       "24780       25294      3            0                   3        0      1   \n",
       "24781       25295      6            0                   6        0      1   \n",
       "24782       25296      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "...                                                  ...  \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24779  you've gone and broke the wrong heart baby, an...  \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781              youu got wild bitches tellin you lies  \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[24783 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EXERCISE: \n",
    "## you can try this on Davidson data\n",
    "davidson_sentence_file =  \"../day1_annotation_interrateragreement/labeled_data.csv\"\n",
    "davidson_df = pd.read_csv(davidson_sentence_file) \n",
    "davidson_df\n",
    "\n",
    "## Note that it has multiclass label:\n",
    "## class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cdb6f48ad7023a573f3b4ac02d894012f19afa4ab0de793319e6ac228b4d09b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
