{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC MODELING\n",
    "####  Source of the following code :a super simplified implementation:  https://bennett-holiday.medium.com/a-step-by-step-guide-to-writing-an-lda-program-in-python-690aa99119ea\n",
    "#### more eloborate one can be found here: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp39-cp39-macosx_11_0_arm64.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.1-cp39-cp39-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.1 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U gensim\n",
    "#!pip install -U nltk\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ozgealacam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_sentence_file =  \"../day1_annotation_interrateragreement/labeled_data.csv\"\n",
    "davidson_df = pd.read_csv(davidson_sentence_file) \n",
    "davidson_text= davidson_df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(documents):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # Tokenize and remove stopwords\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in documents]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = preprocess_data(davidson_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the LDA model\n",
    "# Set number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus, \n",
    "                     id2word=id2word, \n",
    "                     num_topics=num_topics, \n",
    "                     random_state=42, \n",
    "                     passes=10, \n",
    "                     alpha=\"auto\", \n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.069*\"co\" + 0.066*\"http\" + 0.045*\"pussy\" + 0.030*\"rt\" + 0.009*\"bird\" + '\n",
      "  '0.007*\"take\" + 0.006*\"eat\" + 0.005*\"yellow\" + 0.005*\"yankees\" + '\n",
      "  '0.005*\"https\"'),\n",
      " (1,\n",
      "  '0.041*\"trash\" + 0.003*\"racist\" + 0.003*\"place\" + 0.003*\"thirsty\" + '\n",
      "  '0.003*\"white\" + 0.003*\"via\" + 0.003*\"fags\" + 0.003*\"literally\" + '\n",
      "  '0.003*\"giving\" + 0.003*\"brownie\"'),\n",
      " (2,\n",
      "  '0.014*\"stupid\" + 0.011*\"ghetto\" + 0.011*\"big\" + 0.007*\"another\" + '\n",
      "  '0.005*\"wtf\" + 0.004*\"basic\" + 0.004*\"hot\" + 0.004*\"wait\" + 0.004*\"weed\" + '\n",
      "  '0.004*\"niccas\"'),\n",
      " (3,\n",
      "  '0.066*\"bitch\" + 0.058*\"rt\" + 0.027*\"bitches\" + 0.023*\"like\" + 0.020*\"hoes\" '\n",
      "  '+ 0.016*\"hoe\" + 0.012*\"ass\" + 0.011*\"get\" + 0.011*\"shit\" + 0.010*\"nigga\"'),\n",
      " (4,\n",
      "  '0.015*\"trash\" + 0.013*\"gt\" + 0.011*\"charlie\" + 0.005*\"bc\" + 0.005*\"throw\" + '\n",
      "  '0.005*\"retard\" + 0.005*\"everybody\" + 0.005*\"hair\" + 0.005*\"years\" + '\n",
      "  '0.004*\"dyke\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the keywords for each topic\n",
    "pprint(lda_model.print_topics())\n",
    "## Maybe this simple text preprocessing is not enough for social media tweets? What do you think?\n",
    "## Maybe k does not the have the optimal value\n",
    "## Maybe training parameters need to be adjusted\n",
    "## Maybe the models needs more data (LDA is an unsupervised model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ozgealacam/Library/CloudStorage/OneDrive-Personal/00_LMU/HateSpeech23_24/lecture_content/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.4375253917831435\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model using the coherence score\n",
    "#Higher coherence scores indicate a better model. You can try with a set of different k to choose the best model!\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=id2word, coherence=\"c_v\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"Coherence Score: \", coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
